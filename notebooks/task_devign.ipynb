{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bba13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XinZhou\\anaconda3\\envs\\dna\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\XinZhou\\anaconda3\\envs\\dna\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "\n",
    "# packages\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "from datasets import Dataset\n",
    "import pyarrow as pa\n",
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from tokenizers.processors import BertProcessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c6a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPORTANT\n",
    "# out_dir = './task_devign_v3'\n",
    "# raw_data_path = '/scratch/xin/devign_task/tokenized_devign_v3.pkl'\n",
    "MAX_LENGTH = 512\n",
    "batch_size = 8\n",
    "# model_path = '/scratch/xin/bert_source_v3/pretrained-bert/'\n",
    "\n",
    "num_epochs = 10\n",
    "max_step_saves = 5\n",
    "save_step_at = 500\n",
    "test_only = False\n",
    "\n",
    "data_dir = \"..\\\\..\\\\cbert\\\\test_data\\\\devign\\\\\"\n",
    "#     raw_data_path = os.path.join(data_dir, 'devign_all.pkl')\n",
    "#     bert_model_path = '..\\\\..\\\\cbert\\\\tasks\\\\roberta_usage\\\\checkpoint-50000'\n",
    "#     out_dir = 'result_devign'\n",
    "#     tokenizer_path = '..\\\\..\\\\cbert\\\\tasks\\\\roberta_usage'\n",
    "#     data_dir = \"../../vul_dataset/tasks/devign\"\n",
    "raw_data_path = os.path.join(data_dir, 'devign_all.pkl')\n",
    "bert_model_path = '..\\\\..\\\\cbert\\\\pretrained\\\\test_albert_NaCUT\\\\checkpoint'\n",
    "out_dir = 'result_devign'\n",
    "tokenizer_path = '..\\\\..\\\\cbert\\\\pretrained\\\\test_albert_NaCUT\\\\tokenizer.model'\n",
    "\n",
    "truncate_longer_samples = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf84464e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "def get_data(file_path):\n",
    "    raw_df = pd.read_pickle(file_path)  \n",
    "    df = {'filename': [], 'text': [], 'label': []}\n",
    "    for i in range(len(raw_df['code'][:1000])):\n",
    "        df['filename'].append(raw_df['filename'][i])\n",
    "        code = raw_df['code'][i].split(';')\n",
    "        code = ' '.join(code)\n",
    "        df['text'].append(code)\n",
    "        df['label'].append(raw_df['label'][i])\n",
    "    df = pd.DataFrame(df)\n",
    "    return df\n",
    "    ### convert to Huggingface dataset\n",
    "#     return Dataset(pa.Table.from_pandas(df))\n",
    "\n",
    "# train_data = get_data('/scratch/xin/bert_source/mvdsc/tokenized_mvdsc_train.pkl')\n",
    "# valid_data = get_data('/scratch/xin/bert_source/mvdsc/tokenized_mvdsc_valid.pkl')\n",
    "\n",
    "tokenized_data = get_data(raw_data_path)\n",
    "print(len(tokenized_data))\n",
    "# print(tokenized_data['text'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba08f49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 3)\n",
      "(100, 3)\n",
      "(100, 3)\n",
      "652    translation_unit static primitive_type void fu...\n",
      "579    translation_unit int function_declarator ff_in...\n",
      "836    translation_unit av_cold ERROR int function_de...\n",
      "586    translation_unit static primitive_type void fu...\n",
      "226    translation_unit static primitive_type void fu...\n",
      "763    translation_unit static type_identifier ExitSt...\n",
      "695    translation_unit static type_qualifier const p...\n",
      "550    translation_unit static primitive_type int fun...\n",
      "927    translation_unit static primitive_type void fu...\n",
      "515    translation_unit int function_declarator DMA_r...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(tokenized_data, test_size=0.2, shuffle = True, random_state = 8)\n",
    "# train, validation, test = np.split(tokenized_data, [int(.8*len(tokenized_data)), int(.9*len(tokenized_data))])\n",
    "train_data, valid_data, test_data = np.split(tokenized_data.sample(frac=1, random_state=2022), \n",
    "               [int(.8*len(tokenized_data)), int(.9*len(tokenized_data))])\n",
    "\n",
    "# train_data = train_data[:100]\n",
    "# valid_data = valid_data[:100]\n",
    "# test_data = test_data[:100]\n",
    "\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_data['text'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "540c7751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = pd.DataFrame(train_data['label'].value_counts())\n",
    "label_counts\n",
    "label_values = list(label_counts.index)\n",
    "order = list(pd.DataFrame(train_data['label'].value_counts()).index)\n",
    "label_values = [l for _,l in sorted(zip(order, label_values))]\n",
    "label_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d60c19dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translation_unit static primitive_type void function_declarator lance_mem_write argument_list ( primitive_type void init_declarator * identifier voidPointerParamCall , parameter_declaration target_phys_addr_t identifier otherParamPass , parameter_declaration uint64_t identifier otherParamPass1 , parameter_declaration unsigned identifier otherParam ) compound_statement { declaration SysBusPCNetState init_declarator * identifier SysBusPCNetStatePointerDefUsePass = identifier voidPointerParamCall   expression_statement trace_lance_mem_writew argument_list ( identifier otherParamPass , identifier otherParamPass1 pointer_expression & number_literal 6 5 5 3 5 )   expression_statement pcnet_ioport_writew argument_list ( pointer_expression & identifier SysBusPCNetStatePointerDefUsePass -> field_identifier state , identifier otherParamPass , identifier otherParamPass1 pointer_expression & number_literal 6 5 5 3 5 )   }\n"
     ]
    }
   ],
   "source": [
    "train_texts = train_data['text'].values\n",
    "train_labels = train_data['label'].values\n",
    "\n",
    "valid_texts = valid_data['text'].values\n",
    "valid_labels = valid_data['label'].values\n",
    "\n",
    "test_texts = test_data['text'].values\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "print(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7778bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:  11\n",
      "number of zeros:  0\n",
      "max:  4468\n",
      "avg:  493.92375\n",
      "median:  247\n",
      "95%:  1661\n",
      "493.92375\n",
      "661.8347410543819\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "train_texts[0]\n",
    "text_lengths = [len(train_texts[i].split()) for i in range(len(train_texts))]\n",
    "print('min: ', min(text_lengths))\n",
    "print('number of zeros: ', len([item for item in text_lengths if item == 0]))\n",
    "print('max: ', max(sorted(text_lengths)))\n",
    "print('avg: ', sum(text_lengths) / len(text_lengths))\n",
    "print('median: ', sorted(text_lengths)[len(text_lengths) // 2])\n",
    "print('95%: ', sorted(text_lengths)[int(len(text_lengths)*0.95)])\n",
    "print(statistics.mean(text_lengths))\n",
    "print(statistics.stdev(text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3e75ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XinZhou\\AppData\\Local\\Temp\\ipykernel_9620\\2095679928.py:8: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(text_lengths, hist=True, kde=False,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'counts')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# matplotlib histogram\n",
    "plt.hist(text_lengths, color = 'blue', edgecolor = 'black',\n",
    "         bins = int(180/5))\n",
    "# seaborn histogram\n",
    "sns.distplot(text_lengths, hist=True, kde=False, \n",
    "             bins=int(180/5), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "# Add labels\n",
    "plt.title('Histogram of text length analysis')\n",
    "plt.xlabel('text length')\n",
    "plt.ylabel('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d74ba9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for i in range(len(text_lengths)) if text_lengths[i] >= MAX_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d075664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XinZhou\\anaconda3\\envs\\dna\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1678: FutureWarning: Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "loading file ..\\..\\cbert\\pretrained\\test_albert_NaCUT\\tokenizer.model\n",
      "Adding [CLS] to the vocabulary\n",
      "Adding [SEP] to the vocabulary\n",
      "Adding <pad> to the vocabulary\n",
      "Adding [MASK] to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\cbert\\pretrained\\test_albert_NaCUT\\tokenizer.model\n",
      "tokenizer loaded ...\n",
      "vocab size:  32000\n"
     ]
    }
   ],
   "source": [
    "# ========================= load tokenizer START =========================\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "# vocab_size = 32010\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512 # 768\n",
    "import transformers \n",
    "print(tokenizer_path)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer = transformers.AlbertTokenizer.from_pretrained(tokenizer_path)\n",
    "print('tokenizer loaded ...')\n",
    "print('vocab size: ', tokenizer.vocab_size)\n",
    "vocab_size = tokenizer.vocab_size + 10\n",
    "# ========================= load tokenizer END =========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19161df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d7f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_ids = [tokenizer.encode(text, max_length=300, pad_to_max_length=True) for text in texts]\n",
    "if not truncate_longer_samples:\n",
    "    #tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "    train_text_dict = [tokenizer(t, return_special_tokens_mask=True) for t in train_texts]\n",
    "    valid_text_dict = [tokenizer(t, return_special_tokens_mask=True) for t in valid_texts]\n",
    "    test_text_dict = [tokenizer(t, return_special_tokens_mask=True) for t in test_texts]\n",
    "else:\n",
    "    max_length = MAX_LENGTH\n",
    "    train_text_dict = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=max_length, return_special_tokens_mask=True) for t in train_texts]\n",
    "    valid_text_dict = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=max_length, return_special_tokens_mask=True) for t in valid_texts]\n",
    "    test_text_dict = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=max_length, return_special_tokens_mask=True) for t in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ebe6c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_ids = [text_id['input_ids'] for text_id in train_text_dict]\n",
    "valid_text_ids = [text_id['input_ids'] for text_id in valid_text_dict]\n",
    "test_text_ids = [text_id['input_ids'] for text_id in test_text_dict]\n",
    "# text_ids[0]\n",
    "train_att_masks = [text_id['attention_mask'] for text_id in train_text_dict]\n",
    "valid_att_masks = [text_id['attention_mask'] for text_id in valid_text_dict]\n",
    "test_att_masks = [text_id['attention_mask'] for text_id in test_text_dict]\n",
    "# att_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd2efc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, train_m, train_y = train_text_ids, train_att_masks, train_labels\n",
    "val_x, val_m, val_y = valid_text_ids, valid_att_masks, valid_labels\n",
    "test_x, test_m, test_y = test_text_ids, test_att_masks, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c02640a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 512])\n",
      "torch.Size([100, 512])\n",
      "torch.Size([100, 512])\n",
      "torch.Size([800])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([800, 512])\n",
      "torch.Size([100, 512])\n",
      "torch.Size([100, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_x = torch.tensor(train_x)\n",
    "test_x = torch.tensor(test_x)\n",
    "val_x = torch.tensor(val_x)\n",
    "train_y = torch.tensor(train_y)\n",
    "test_y = torch.tensor(test_y)\n",
    "val_y = torch.tensor(val_y)\n",
    "train_m = torch.tensor(train_m)\n",
    "test_m = torch.tensor(test_m)\n",
    "val_m = torch.tensor(val_m)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(val_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)\n",
    "print(val_y.shape)\n",
    "print(train_m.shape)\n",
    "print(test_m.shape)\n",
    "print(val_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea48ae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32000,    50,     3,  ..., 32002, 32002, 32002],\n",
      "        [32000,    50,     3,  ...,     8,    32, 32001],\n",
      "        [32000,    50,     3,  ...,   431,  1567, 32001],\n",
      "        ...,\n",
      "        [32000,    50,     3,  ...,  5152,   952, 32001],\n",
      "        [32000,    50,     3,  ..., 32002, 32002, 32002],\n",
      "        [32000,    50,     3,  ...,     8,    93, 32001]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "print(train_x)\n",
    "train_data = TensorDataset(train_x, train_m, train_y)\n",
    "train_sampler = list(range(len(train_data)))\n",
    "\n",
    "\n",
    "# train_sampler = train_sampler[:100]\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "val_data = TensorDataset(val_x, val_m, val_y)\n",
    "val_sampler = list(range(len(val_data)))\n",
    "# val_sampler = val_sampler[:100]\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# train_loader_extracted = torch.utils.data.DataLoader(train_extracted_data, batch_size=batch_size, shuffle=False,\n",
    "#                                            sampler=train_sampler)\n",
    "\n",
    "\n",
    "# valid_loader_extracted = torch.utils.data.DataLoader(valid_extracted_data, batch_size=batch_size, shuffle=False,\n",
    "#                                            sampler=val_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c02a28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "\n",
    "class DotAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn_vector = nn.Parameter(\n",
    "            torch.Tensor(1, hidden_size), requires_grad=True)\n",
    "\n",
    "        init.xavier_uniform(self.attn_vector.data)\n",
    "\n",
    "    def get_mask(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs, lengths=None):\n",
    "        batch_size, max_len = inputs.size()[:2]\n",
    "\n",
    "        '''\n",
    "        print(\"INPUTS\", inputs.size())\n",
    "        print(\"ATTN\", self.attn_vector  # (1, hidden_size)\n",
    "                            .unsqueeze(0)  # (1, hidden_size, 1)\n",
    "                            .transpose(2, 1)\n",
    "                            .repeat(batch_size, 1, 1).size())'''\n",
    "        # apply attention layer\n",
    "        weights = torch.bmm(inputs,\n",
    "                            self.attn_vector  # (1, hidden_size)\n",
    "                            .unsqueeze(0)  # (1, 1, hidden_size)\n",
    "                            .transpose(2, 1) # (1, hidden_size, 1)\n",
    "                            .repeat(batch_size, 1, 1)) # (batch_size, hidden_size, 1))\n",
    "\n",
    "        attn_energies = F.softmax(F.relu(weights.squeeze(axis=2)))\n",
    "\n",
    "        # create mask based on the sentence lengths\n",
    "        #idxes = torch.arange(0, max_len, out=torch.LongTensor(max_len)).unsqueeze(0).cuda()  # some day, you'll be able to directly do this on cuda\n",
    "        #mask = Variable((idxes < lengths.data.unsqueeze(1)).float())\n",
    "\n",
    "        # apply mask and renormalize attention scores (weights)\n",
    "        #masked = attn_weights * mask\n",
    "        _sums = attn_energies.sum(-1).unsqueeze(1).expand_as(attn_energies)  # sums per row\n",
    "        attn_weights = attn_energies / _sums\n",
    "\n",
    "        # print('attn_weights: ', attn_weights.shape)\n",
    "        # apply attention weights\n",
    "\n",
    "        weighted = torch.mul(inputs, attn_weights.unsqueeze(-1).expand_as(inputs))\n",
    "\n",
    "        # print('weighted: ', weighted.shape)\n",
    "\n",
    "        # get the final fixed vector representations of the sentences\n",
    "        representations = weighted.sum(1).squeeze()\n",
    "        \n",
    "        if len(representations.shape) == 1:\n",
    "            representations = representations.reshape(1, representations.shape[-1])\n",
    "        # print('representations: ', representations.shape)\n",
    "\n",
    "        return representations, attn_weights\n",
    "    \n",
    "from transformers import AutoModelForSequenceClassification\n",
    "    \n",
    "class BERTSourceBinaryClassification(nn.Module):\n",
    "    def __init__(self,checkpoint, num_labels): \n",
    "        super(BERTSourceBinaryClassification,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "#         self.model = BertForMaskedLM.from_pretrained(checkpoint)\n",
    "        #Load Model with given checkpoint and extract its body\n",
    "        # self.model = AutoModelForSequenceClassification.from_pretrained(bert_model_path, num_labels=2)\n",
    "        self.model  = AutoModel.from_pretrained(checkpoint, output_attentions=True, output_hidden_states=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.features = nn.Linear(56, 56)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.classifier = nn.Linear(768 * 4, num_labels) # load and initialize weights\n",
    "        self.out = nn.Softmax()\n",
    "        self.attn = DotAttention(hidden_size = 768)\n",
    "    \n",
    "    def forward_single(self, input_ids, attention_mask, labels, ex):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)[0]\n",
    "        # print(outputs)\n",
    "        # outputs = outputs\n",
    "        # hidden_states, pool_output, unknown, attention_weights\n",
    "        pooler_output = outputs[:, 0, :]\n",
    "        # max + last + attention START\n",
    "        last =  outputs[:,-1] \n",
    "        attn, attention_weights = self.attn.forward(outputs)\n",
    "        max_ , _ = torch.max(outputs, dim=1)\n",
    "        concatenated = torch.cat([last, max_, attn, pooler_output], dim=1)\n",
    "        # Add custom layers\n",
    "        sequence_output = self.dropout(concatenated) #outputs[0]=last hidden state\n",
    "        logits = self.classifier(sequence_output) # calculate losses\n",
    "        #return logits, attention_weights\n",
    "        #loss = None\n",
    "        #if labels is not None:\n",
    "            #loss_fct = nn.CrossEntropyLoss()\n",
    "            #loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return logits, attention_weights\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, ex=None, max_len=MAX_LENGTH, opt='mean'):\n",
    "        #Extract outputs from the body\n",
    "        final_logits = None\n",
    "        final_attention_weights = None\n",
    "        # pick the most suspicous one for final decision\n",
    "        for i in range(0, max_len, 512):\n",
    "            logits, attention_weights = self.forward_single(input_ids[:,i:i+512], attention_mask[:,i:i+512], labels, ex)\n",
    "            if final_logits is None:\n",
    "                final_logits = logits\n",
    "                final_attention_weights = attention_weights\n",
    "            else:\n",
    "                final_logits = torch.cat([final_logits,  logits], dim=1)\n",
    "                final_attention_weights = torch.cat([final_attention_weights,  attention_weights], dim=1)\n",
    "            #loss, logits = output\n",
    "        \n",
    "        final_logits = final_logits.reshape(final_logits.shape[0], max_len//512, 2)\n",
    "        \n",
    "        if opt == 'mean':\n",
    "            # mean \n",
    "            final_logits = torch.mean(final_logits, dim=(1))\n",
    "        else:\n",
    "            # max\n",
    "            final_logits = torch.amax(final_logits, dim=(1))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(final_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        final_logits = self.softmax(final_logits)\n",
    "        return (loss, final_logits), final_attention_weights\n",
    "\n",
    "\n",
    "        #return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43f8ca24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ..\\..\\cbert\\pretrained\\test_albert_NaCUT\\checkpoint\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"..\\\\..\\\\cbert\\\\pretrained\\\\test_albert_NaCUT\\\\checkpoint\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32010\n",
      "}\n",
      "\n",
      "loading weights file ..\\..\\cbert\\pretrained\\test_albert_NaCUT\\checkpoint\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\cbert\\pretrained\\test_albert_NaCUT\\checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ..\\..\\cbert\\pretrained\\test_albert_NaCUT\\checkpoint were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertModel were not initialized from the model checkpoint at ..\\..\\cbert\\pretrained\\test_albert_NaCUT\\checkpoint and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\XinZhou\\AppData\\Local\\Temp\\ipykernel_9620\\1294117063.py:17: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.attn_vector.data)\n"
     ]
    }
   ],
   "source": [
    "print(bert_model_path)\n",
    "model = BERTSourceBinaryClassification(checkpoint=bert_model_path, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8492af36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XinZhou\\anaconda3\\envs\\dna\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 1e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.2},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd3aa44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "multi_gpu = torch.cuda.device_count() \n",
    "if multi_gpu > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = nn.DataParallel(model)\n",
    "# model = nn.DataParallel(model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ded1e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aa7865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations:  100\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed_val = 111\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "# save model \n",
    "\n",
    "def save_model(model, train_losses, val_losses, out_dir_model):\n",
    "\n",
    "    if not os.path.exists(out_dir_model):\n",
    "        os.makedirs(out_dir_model)\n",
    "    \n",
    "    PATH = os.path.join(out_dir_model, 'BERTSourceBinaryClassification.pt')\n",
    "\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    with open(out_dir_model + '/train_losses.pkl', 'wb') as f:\n",
    "        pickle.dump(train_losses, f)\n",
    "\n",
    "    with open(out_dir_model + '/val_losses.pkl', 'wb') as f:\n",
    "        pickle.dump(val_losses, f)\n",
    "        \n",
    "print('number of iterations: ', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4eca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64535cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\XinZhou\\AppData\\Local\\Temp\\ipykernel_9620\\1294117063.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attn_energies = F.softmax(F.relu(weights.squeeze(axis=2)))\n",
      "C:\\Users\\XinZhou\\AppData\\Local\\Temp\\ipykernel_9620\\1294117063.py:133: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_logits = self.softmax(final_logits)\n",
      "100it [00:44,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss after itaration 1: 0.700383\n",
      "Validation loss after itaration 1: 0.675975\n",
      "val_losses:  [] 0.6759752631187439\n",
      "Time: 0m 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:45,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss after itaration 2: 0.715992\n",
      "Validation loss after itaration 2: 0.672396\n",
      "val_losses:  [0.6759752631187439] 0.6723963022232056\n",
      "the best model updated\n",
      "saved as:  ./result_devign/classification_model_best\n",
      "Time: 0m 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:42,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss after itaration 3: 0.710995\n",
      "Validation loss after itaration 3: 0.673746\n",
      "val_losses:  [0.6759752631187439, 0.6723963022232056] 0.67374587059021\n",
      "Time: 0m 44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:38,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss after itaration 4: 0.707245\n",
      "Validation loss after itaration 4: 0.675893\n",
      "val_losses:  [0.6759752631187439, 0.6723963022232056, 0.67374587059021] 0.6758934259414673\n",
      "Time: 0m 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:09,  2.11it/s]"
     ]
    }
   ],
   "source": [
    "if not test_only:\n",
    "    from tqdm import tqdm\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    num_mb_train = len(train_dataloader)\n",
    "    num_mb_val = len(val_dataloader)\n",
    "    current_step = 0\n",
    "    saved_steps = []\n",
    "    if num_mb_val == 0:\n",
    "        num_mb_val = 1\n",
    "\n",
    "    # print('number of iterations: ', len(train_dataloader), len(train_loader_extracted))\n",
    "    print('num_epochs: ', num_epochs)\n",
    "\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    for n in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        #for k, (data1, data2) in enumerate(zip(train_dataloader, train_loader_extracted)):\n",
    "        for k, (mb_x, mb_m, mb_y) in tqdm(enumerate(train_dataloader)):\n",
    "            #mb_x, mb_m, mb_y = data1\n",
    "            # 56 features\n",
    "            #ex, ey, e_index = data2\n",
    "            optimizer.zero_grad()\n",
    "            model.train()\n",
    "\n",
    "            mb_x = mb_x.cuda()\n",
    "            mb_m = mb_m.cuda()\n",
    "            mb_y = mb_y.cuda()\n",
    "            #ex = ex.cuda()\n",
    "\n",
    "            outputs, _, = model(mb_x, attention_mask=mb_m, labels=mb_y)\n",
    "            #print(outputs[1])\n",
    "            #mb_y = F.one_hot(mb_y)\n",
    "            #print(outputs, mb_y)\n",
    "            #loss = loss_fct(outputs.view(-1, 2), mb_y.view(-1))\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if multi_gpu > 1:\n",
    "                loss = loss.sum()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            current_step += 1\n",
    "            train_loss += loss.data / num_mb_train\n",
    "\n",
    "            if current_step % save_step_at == 0:\n",
    "                saved_steps.append(current_step)\n",
    "                out_dir_model = './{}/classification_model_{}'.format(out_dir, saved_steps[-1])\n",
    "                print('saved as: ', out_dir_model)\n",
    "                if len(saved_steps) > max_step_saves:\n",
    "                    delete_dir = './{}/classification_model_{}'.format(out_dir, saved_steps[-(max_step_saves+1)])\n",
    "                    os.system('rm -r {}'.format(delete_dir))\n",
    "                save_model(model, train_losses, val_losses, out_dir_model)\n",
    "\n",
    "\n",
    "        print (\"\\nTrain loss after itaration %i: %f\" % (n+1, train_loss))\n",
    "        train_losses.append(train_loss.cpu())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for k, (mb_x, mb_m, mb_y) in enumerate(val_dataloader):\n",
    "                mb_x = mb_x.cuda()\n",
    "                mb_m = mb_m.cuda()\n",
    "                mb_y = mb_y.cuda()\n",
    "\n",
    "                outputs, _ = model(mb_x, attention_mask=mb_m, labels=mb_y)\n",
    "                loss = outputs[0]\n",
    "                if multi_gpu > 1:\n",
    "                    loss = loss.sum()\n",
    "                #loss = model_loss(outputs[1], mb_y)\n",
    "\n",
    "                val_loss += loss.data / num_mb_val\n",
    "\n",
    "            print (\"Validation loss after itaration %i: %f\" % (n+1, val_loss))\n",
    "            # save the best\n",
    "            print('val_losses: ', val_losses, float(val_loss.cpu()))\n",
    "            if val_losses and float(val_loss.cpu()) <= min(val_losses):\n",
    "                out_dir_model = './{}/classification_model_best'.format(out_dir)\n",
    "                print('the best model updated')\n",
    "                print('saved as: ', out_dir_model)\n",
    "                save_model(model, train_losses, val_losses, out_dir_model)\n",
    "            val_losses.append(float(val_loss.cpu()))\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        print(f'Time: {epoch_mins}m {epoch_secs}s')\n",
    "        if len(val_losses) > 3 and float(val_loss.cpu()) > max(val_losses[-4:-1]):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b99602",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# out_dir_model = '{}/classification_model_4000'.format(out_dir)\n",
    "\n",
    "test_dir_model = '{}/classification_model_500'.format(out_dir)\n",
    "\n",
    "PATH = os.path.join(test_dir_model, 'BERTSourceBinaryClassification.pt')\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "\n",
    "with open(test_dir_model + '/train_losses.pkl', 'rb') as f:\n",
    "    train_losses = pickle.load(f)\n",
    "    \n",
    "with open(test_dir_model + '/val_losses.pkl', 'rb') as f:\n",
    "    val_losses = pickle.load(f)\n",
    "\n",
    "print(val_losses)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# test_data = test_data[:100]\n",
    "test_data = TensorDataset(test_x, test_m)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "outputs = []\n",
    "print('num of iterations: ', len(test_dataloader))\n",
    "with torch.no_grad():\n",
    "    for k, (mb_x, mb_m) in tqdm(enumerate(test_dataloader)):\n",
    "        mb_x = mb_x.cuda()\n",
    "        mb_m = mb_m.cuda()\n",
    "#         output, _ = model(mb_x)\n",
    "        output, attention_weights = model(mb_x, attention_mask=mb_m, max_len=MAX_LENGTH, opt='mean')\n",
    "#         print(output)\n",
    "#         print(attention_weights[4])\n",
    "#         sys.exit(0)\n",
    "        outputs.append(output[1].to('cpu'))\n",
    "        \n",
    "outputs = torch.cat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted_values = torch.max(outputs, 1)\n",
    "predicted_values = predicted_values.numpy()\n",
    "true_values = test_y.numpy()\n",
    "\n",
    "test_accuracy = np.sum(predicted_values == true_values) / len(true_values)\n",
    "print (\"Test Accuracy:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# plot confusion matrix\n",
    "# code borrowed from scikit-learn.org\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cm_test = confusion_matrix(true_values, predicted_values)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(cm_test, classes=label_values, title='Confusion Matrix - Test Dataset')\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(cm_test, classes=label_values, title='Confusion Matrix - Test Dataset', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5d314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c4989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
