07/08/2022 23:54:15 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 4, distributed training: False, 16-bits training: False
07/08/2022 23:54:17 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../../../cbert/train.jsonl', output_dir='./saved_models', eval_data_file='../../../cbert/valid.jsonl', test_data_file='../../../cbert/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base', mlm=False, mlm_probability=0.15, config_name='microsoft/codebert-base', tokenizer_name='roberta-base', cache_dir='', block_size=400, do_train=True, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=12, eval_batch_size=16, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=4, device=device(type='cuda'), per_gpu_train_batch_size=3, per_gpu_eval_batch_size=4, start_epoch=0, start_step=0)
07/08/2022 23:54:54 - INFO - __main__ -   *** Example ***
07/08/2022 23:54:54 - INFO - __main__ -   idx: 0
07/08/2022 23:54:54 - INFO - __main__ -   label: 1
07/08/2022 23:54:54 - INFO - __main__ -   input_tokens: ['[CLS]', 'void', 'externxid', 'lparen', 'int', 'externxidxextern', 'comma', 'int', 'externxidxle', 'rparen', 'lbrace', 'int', 'lextxtxexternxid', 'semi', 'if', 'lparen', 'externxidxextern', 'eq', '1', 'rparen', 'externxidxextern', 'plusplus', 'semi', 'for', 'lparen', 'lextxtxexternxid', 'equals', 'externxidxle', 'semi', 'lextxtxexternxid', 'le', 'externxidxextern', 'semi', 'lextxtxexternxid', 'plusplus', 'rparen', 'lbrace', 'if', 'lparen', 'externxidxextern', 'mod', 'lextxtxexternxid', 'eq', '0', 'rparen', 'externxid', 'lparen', 'externxidxextern', 'divide', 'lextxtxexternxid', 'comma', 'lextxtxexternxid', 'rparen', 'semi', 'rbrace', 'rbrace', 'int', 'externxid', 'lparen', 'rparen', 'lbrace', 'int', 'externxidxextern', 'comma', 'externxidxextern', 'comma', 'txlextxexternxid', 'semi', 'int', 'externxidxextern', 'equals', '2', 'semi', 'int', 'externxidxextern', 'lbracket', '1', '0', '0', 'rbracket', 'semi', 'externxidxextern', 'rshift', 'externxidxextern', 'semi', 'for', 'lparen', 'txlextxexternxid', 'equals', '1', 'semi', 'txlextxexternxid', 'le', 'externxidxextern', 'semi', 'txlextxexternxid', 'plusplus', 'rparen', 'lbrace', 'externxidxextern', 'rshift', 'externxidxextern', 'semi', 'externxidxextern', 'lparen', 'externxidxextern', 'comma', 'externxidxextern', 'rparen', 'semi', 'externxidxextern', 'lbracket', 'txlextxexternxid', 'rbracket', 'equals', 'externxidxextern', 'semi', 'externxidxextern', 'equals', '0', 'semi', 'rbrace', 'for', 'lparen', 'txlextxexternxid', 'equals', '1', 'semi', 'txlextxexternxid', 'le', 'externxidxextern', 'semi', 'txlextxexternxid', 'plusplus', 'rparen', 'externxidxextern', 'lshift', 'externxidxextern', 'lbracket', 'txlextxexternxid', 'rbracket', 'lshift', 'externxidxextern', 'semi', 'return', '0', 'semi', 'rbrace', 'int', 'unkid', 'equals', '0', 'semi', '[SEP]']
07/08/2022 23:54:54 - INFO - __main__ -   input_ids: 2 173 282 108 129 136 101 129 2645 107 175 129 4018 115 167 108 136 154 12 107 136 374 115 350 108 4018 159 2645 115 4018 606 136 115 4018 374 107 175 167 108 136 671 4018 154 11 107 282 108 136 237 4018 101 4018 107 115 189 189 129 282 108 107 175 129 136 101 136 101 2131 115 129 136 159 13 115 129 136 214 12 11 11 215 115 136 433 136 115 350 108 2131 159 12 115 2131 606 136 115 2131 374 107 175 136 433 136 115 136 108 136 101 136 107 115 136 214 2131 215 159 136 115 136 159 11 115 189 350 108 2131 159 12 115 2131 606 136 115 2131 374 107 136 291 136 214 2131 215 291 136 115 206 11 115 189 129 127 159 11 115 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
07/08/2022 23:54:54 - INFO - __main__ -   *** Example ***
07/08/2022 23:54:54 - INFO - __main__ -   idx: 1
07/08/2022 23:54:54 - INFO - __main__ -   label: 1
07/08/2022 23:54:54 - INFO - __main__ -   input_tokens: ['[CLS]', 'int', 'externxid', 'lparen', 'int', 'externxidxextern', 'comma', 'int', 'txlextxexternxid', 'rparen', 'lbrace', 'for', 'lparen', 'txlextxexternxid', 'plusplus', 'semi', 'txlextxexternxid', 'le', 'externxidxextern', 'semi', 'txlextxexternxid', 'plusplus', 'rparen', 'lbrace', 'if', 'lparen', 'externxidxextern', 'eq', 'txlextxexternxid', 'rparen', 'return', '1', 'semi', 'if', 'lparen', 'externxidxextern', 'mod', 'txlextxexternxid', 'eq', '0', 'land', 'externxidxextern', 'divide', 'txlextxexternxid', 'ge', 'txlextxexternxid', 'rparen', 'return', 'externxid', 'lparen', 'externxidxextern', 'divide', 'txlextxexternxid', 'comma', 'txlextxexternxid', 'minus', '1', 'rparen', 'plus', 'externxid', 'lparen', 'externxidxextern', 'comma', 'txlextxexternxid', 'rparen', 'semi', 'rbrace', 'return', '0', 'semi', 'rbrace', 'int', 'externxid', 'lparen', 'rparen', 'lbrace', 'int', 'externxidxextern', 'comma', 'externxidxextern', 'semi', 'externxidxextern', 'rshift', 'externxidxextern', 'semi', 'for', 'lparen', 'int', 'txlextxexternxid', 'equals', '1', 'semi', 'txlextxexternxid', 'le', 'externxidxextern', 'semi', 'txlextxexternxid', 'plusplus', 'rparen', 'lbrace', 'externxidxextern', 'rshift', 'externxidxextern', 'semi', 'externxidxextern', 'lshift', 'externxidxextern', 'lparen', 'externxidxextern', 'comma', '1', 'rparen', 'lshift', 'externxidxextern', 'semi', 'rbrace', 'return', '0', 'semi', 'rbrace', '[SEP]']
07/08/2022 23:54:54 - INFO - __main__ -   input_ids: 2 129 282 108 129 136 101 129 2131 107 175 350 108 2131 374 115 2131 606 136 115 2131 374 107 175 167 108 136 154 2131 107 206 12 115 167 108 136 671 2131 154 11 357 136 237 2131 510 2131 107 206 282 108 136 237 2131 101 2131 245 12 107 232 282 108 136 101 2131 107 115 189 206 11 115 189 129 282 108 107 175 129 136 101 136 115 136 433 136 115 350 108 129 2131 159 12 115 2131 606 136 115 2131 374 107 175 136 433 136 115 136 291 136 108 136 101 12 107 291 136 115 189 206 11 115 189 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
07/08/2022 23:54:54 - INFO - __main__ -   *** Example ***
07/08/2022 23:54:54 - INFO - __main__ -   idx: 2
07/08/2022 23:54:54 - INFO - __main__ -   label: 1
07/08/2022 23:54:54 - INFO - __main__ -   input_tokens: ['[CLS]', 'int', 'externxid', 'lparen', 'int', 'externxidxle', 'comma', 'int', 'externxidxextern', 'comma', 'int', 'externxidxextern', 'rparen', 'lbrace', 'int', 'externxidxextern', 'equals', '0', 'comma', 'plusxtxtxexternxid', 'semi', 'if', 'lparen', 'externxidxle', 'gt', 'externxidxextern', 'rparen', 'return', '0', 'semi', 'if', 'lparen', 'externxidxextern', 'eq', '1', 'rparen', 'return', '1', 'semi', 'for', 'lparen', 'plusxtxtxexternxid', 'equals', 'externxidxle', 'semi', 'plusxtxtxexternxid', 'le', 'externxidxextern', 'semi', 'plusxtxtxexternxid', 'plusplus', 'rparen', 'if', 'lparen', 'externxidxextern', 'mod', 'plusxtxtxexternxid', 'eq', '0', 'rparen', 'externxidxextern', 'equals', 'externxidxextern', 'plus', 'externxid', 'lparen', 'plusxtxtxexternxid', 'comma', 'externxidxextern', 'minus', '1', 'comma', 'externxidxextern', 'divide', 'plusxtxtxexternxid', 'rparen', 'semi', 'return', 'externxidxextern', 'semi', 'rbrace', 'int', 'externxid', 'lparen', 'rparen', 'lbrace', 'int', 'externxidxextern', 'comma', 'externxidxextern', 'comma', 'externxidxextern', 'comma', 'externxidxextern', 'equals', '0', 'comma', 'txlextxdividext', 'semi', 'externxidxextern', 'rshift', 'externxidxextern', 'semi', 'while', 'lparen', 'externxidxextern', 'lt', 'externxidxextern', 'rparen', 'lbrace', 'externxidxextern', 'rshift', 'externxidxextern', 'semi', 'externxidxextern', 'equals', '0', 'semi', 'int', 'dividextxtxexternxid', 'equals', 'externxidxextern', 'semi', 'for', 'lparen', 'txlextxdividext', 'equals', '2', 'semi', 'txlextxdividext', 'le', 'dividextxtxexternxid', 'semi', 'txlextxdividext', 'plusplus', 'rparen', 'if', 'lparen', 'dividextxtxexternxid', 'mod', 'txlextxdividext', 'eq', '0', 'rparen', 'lbrace', 'externxidxextern', 'plusplus', 'semi', 'dividextxtxexternxid', 'equals', 'dividextxtxexternxid', 'divide', 'txlextxdividext', 'semi', 'txlextxdividext', 'equals', '1', 'semi', 'rbrace', 'externxidxextern', 'lshift', 'externxidxextern', 'lparen', '1', 'comma', 'externxidxextern', 'comma', 'externxidxextern', 'rparen', 'lshift', 'externxidxextern', 'semi', 'externxidxextern', 'plusplus', 'semi', 'rbrace', 'return', '0', 'semi', 'rbrace', '[SEP]']
07/08/2022 23:54:54 - INFO - __main__ -   input_ids: 2 129 282 108 129 2645 101 129 136 101 129 136 107 175 129 136 159 11 101 801 115 167 108 2645 211 136 107 206 11 115 167 108 136 154 12 107 206 12 115 350 108 801 159 2645 115 801 606 136 115 801 374 107 167 108 136 671 801 154 11 107 136 159 136 232 282 108 801 101 136 245 12 101 136 237 801 107 115 206 136 115 189 129 282 108 107 175 129 136 101 136 101 136 101 136 159 11 101 8307 115 136 433 136 115 453 108 136 196 136 107 175 136 433 136 115 136 159 11 115 129 1872 159 136 115 350 108 8307 159 13 115 8307 606 1872 115 8307 374 107 167 108 1872 671 8307 154 11 107 175 136 374 115 1872 159 1872 237 8307 115 8307 159 12 115 189 136 291 136 108 12 101 136 101 136 107 291 136 115 136 374 115 189 206 11 115 189 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
07/08/2022 23:54:55 - INFO - __main__ -   ***** Running training *****
07/08/2022 23:54:55 - INFO - __main__ -     Num examples = 32000
07/08/2022 23:54:55 - INFO - __main__ -     Num Epochs = 3
07/08/2022 23:54:55 - INFO - __main__ -     Instantaneous batch size per GPU = 3
07/08/2022 23:54:55 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 12
07/08/2022 23:54:55 - INFO - __main__ -     Gradient Accumulation steps = 1
07/08/2022 23:54:55 - INFO - __main__ -     Total optimization steps = 8001
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/xzhou29/miniconda/envs/bert/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
07/09/2022 00:00:18 - INFO - __main__ -   epoch 0 step 100 loss 10.81756
07/09/2022 00:05:36 - INFO - __main__ -   epoch 0 step 200 loss 8.58045
07/09/2022 00:10:57 - INFO - __main__ -   epoch 0 step 300 loss 6.80656
07/09/2022 00:16:18 - INFO - __main__ -   epoch 0 step 400 loss 5.48894
07/09/2022 00:21:39 - INFO - __main__ -   epoch 0 step 500 loss 4.61471
07/09/2022 00:27:00 - INFO - __main__ -   epoch 0 step 600 loss 4.00109
07/09/2022 00:32:21 - INFO - __main__ -   epoch 0 step 700 loss 3.52206
07/09/2022 00:37:42 - INFO - __main__ -   epoch 0 step 800 loss 3.17828
07/09/2022 00:43:03 - INFO - __main__ -   epoch 0 step 900 loss 2.8973
07/09/2022 00:48:24 - INFO - __main__ -   epoch 0 step 1000 loss 2.6626
07/09/2022 00:53:45 - INFO - __main__ -   epoch 0 step 1100 loss 2.47399
07/09/2022 00:59:05 - INFO - __main__ -   epoch 0 step 1200 loss 2.31661
07/09/2022 01:04:26 - INFO - __main__ -   epoch 0 step 1300 loss 2.17746
07/09/2022 01:09:46 - INFO - __main__ -   epoch 0 step 1400 loss 2.05957
07/09/2022 01:15:07 - INFO - __main__ -   epoch 0 step 1500 loss 1.95479
07/09/2022 01:20:28 - INFO - __main__ -   epoch 0 step 1600 loss 1.85862
07/09/2022 01:25:49 - INFO - __main__ -   epoch 0 step 1700 loss 1.77458
07/09/2022 01:31:10 - INFO - __main__ -   epoch 0 step 1800 loss 1.70018
07/09/2022 01:36:30 - INFO - __main__ -   epoch 0 step 1900 loss 1.63325
07/09/2022 01:41:51 - INFO - __main__ -   epoch 0 step 2000 loss 1.5702
07/09/2022 01:47:12 - INFO - __main__ -   epoch 0 step 2100 loss 1.51014
07/09/2022 01:52:33 - INFO - __main__ -   epoch 0 step 2200 loss 1.45923
07/09/2022 01:57:54 - INFO - __main__ -   epoch 0 step 2300 loss 1.40787
07/09/2022 02:03:14 - INFO - __main__ -   epoch 0 step 2400 loss 1.36015
07/09/2022 02:08:35 - INFO - __main__ -   epoch 0 step 2500 loss 1.31931
07/09/2022 02:13:56 - INFO - __main__ -   epoch 0 step 2600 loss 1.27964
07/09/2022 02:17:40 - INFO - __main__ -   ***** Running evaluation *****
07/09/2022 02:17:40 - INFO - __main__ -     Num examples = 8000
07/09/2022 02:17:40 - INFO - __main__ -     Batch size = 16
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
07/09/2022 02:30:05 - INFO - __main__ -     eval_loss = 0.2792
07/09/2022 02:30:05 - INFO - __main__ -     eval_map = 0.6786
07/09/2022 02:30:05 - INFO - __main__ -     ********************
07/09/2022 02:30:05 - INFO - __main__ -     Best map:0.6786
07/09/2022 02:30:05 - INFO - __main__ -     ********************
07/09/2022 02:30:05 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-map/model.bin
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
07/09/2022 02:35:27 - INFO - __main__ -   epoch 1 step 100 loss 0.24427
07/09/2022 02:40:48 - INFO - __main__ -   epoch 1 step 200 loss 0.25722
07/09/2022 02:46:09 - INFO - __main__ -   epoch 1 step 300 loss 0.23598
07/09/2022 02:51:31 - INFO - __main__ -   epoch 1 step 400 loss 0.24929
07/09/2022 02:56:51 - INFO - __main__ -   epoch 1 step 500 loss 0.23737
07/09/2022 03:02:12 - INFO - __main__ -   epoch 1 step 600 loss 0.23413
07/09/2022 03:07:33 - INFO - __main__ -   epoch 1 step 700 loss 0.22364
07/09/2022 03:12:54 - INFO - __main__ -   epoch 1 step 800 loss 0.22464
07/09/2022 03:18:16 - INFO - __main__ -   epoch 1 step 900 loss 0.22617
07/09/2022 03:23:38 - INFO - __main__ -   epoch 1 step 1000 loss 0.22016
07/09/2022 03:29:00 - INFO - __main__ -   epoch 1 step 1100 loss 0.22239
07/09/2022 03:34:22 - INFO - __main__ -   epoch 1 step 1200 loss 0.2293
07/09/2022 03:39:44 - INFO - __main__ -   epoch 1 step 1300 loss 0.22728
07/09/2022 03:45:05 - INFO - __main__ -   epoch 1 step 1400 loss 0.2237
07/09/2022 03:50:26 - INFO - __main__ -   epoch 1 step 1500 loss 0.22646
07/09/2022 03:55:48 - INFO - __main__ -   epoch 1 step 1600 loss 0.22154
07/09/2022 04:01:09 - INFO - __main__ -   epoch 1 step 1700 loss 0.22002
07/09/2022 04:06:31 - INFO - __main__ -   epoch 1 step 1800 loss 0.21918
07/09/2022 04:11:52 - INFO - __main__ -   epoch 1 step 1900 loss 0.21368
07/09/2022 04:17:13 - INFO - __main__ -   epoch 1 step 2000 loss 0.2106
07/09/2022 04:22:34 - INFO - __main__ -   epoch 1 step 2100 loss 0.20966
07/09/2022 04:27:56 - INFO - __main__ -   epoch 1 step 2200 loss 0.20747
07/09/2022 04:33:17 - INFO - __main__ -   epoch 1 step 2300 loss 0.20506
07/09/2022 04:38:39 - INFO - __main__ -   epoch 1 step 2400 loss 0.20282
07/09/2022 04:44:00 - INFO - __main__ -   epoch 1 step 2500 loss 0.19952
07/09/2022 04:49:20 - INFO - __main__ -   epoch 1 step 2600 loss 0.19774
07/09/2022 04:52:55 - INFO - __main__ -   ***** Running evaluation *****
07/09/2022 04:52:55 - INFO - __main__ -     Num examples = 8000
07/09/2022 04:52:55 - INFO - __main__ -     Batch size = 16
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
07/09/2022 05:05:14 - INFO - __main__ -     eval_loss = 0.2038
07/09/2022 05:05:14 - INFO - __main__ -     eval_map = 0.6753
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
07/09/2022 05:10:36 - INFO - __main__ -   epoch 2 step 100 loss 0.07302
07/09/2022 05:15:57 - INFO - __main__ -   epoch 2 step 200 loss 0.1023
07/09/2022 05:21:18 - INFO - __main__ -   epoch 2 step 300 loss 0.10531
07/09/2022 05:26:39 - INFO - __main__ -   epoch 2 step 400 loss 0.11709
07/09/2022 05:32:00 - INFO - __main__ -   epoch 2 step 500 loss 0.12241
07/09/2022 05:37:21 - INFO - __main__ -   epoch 2 step 600 loss 0.13078
07/09/2022 05:42:42 - INFO - __main__ -   epoch 2 step 700 loss 0.13022
07/09/2022 05:48:02 - INFO - __main__ -   epoch 2 step 800 loss 0.13062
07/09/2022 05:53:23 - INFO - __main__ -   epoch 2 step 900 loss 0.13675
07/09/2022 05:58:44 - INFO - __main__ -   epoch 2 step 1000 loss 0.13838
07/09/2022 06:04:04 - INFO - __main__ -   epoch 2 step 1100 loss 0.13535
07/09/2022 06:09:25 - INFO - __main__ -   epoch 2 step 1200 loss 0.13174
07/09/2022 06:14:47 - INFO - __main__ -   epoch 2 step 1300 loss 0.12946
07/09/2022 06:20:08 - INFO - __main__ -   epoch 2 step 1400 loss 0.12796
07/09/2022 06:25:30 - INFO - __main__ -   epoch 2 step 1500 loss 0.12985
07/09/2022 06:30:51 - INFO - __main__ -   epoch 2 step 1600 loss 0.12921
07/09/2022 06:36:12 - INFO - __main__ -   epoch 2 step 1700 loss 0.13044
07/09/2022 06:41:33 - INFO - __main__ -   epoch 2 step 1800 loss 0.12674
07/09/2022 06:46:54 - INFO - __main__ -   epoch 2 step 1900 loss 0.12674
07/09/2022 06:52:15 - INFO - __main__ -   epoch 2 step 2000 loss 0.12645
07/09/2022 06:57:37 - INFO - __main__ -   epoch 2 step 2100 loss 0.12696
07/09/2022 07:02:58 - INFO - __main__ -   epoch 2 step 2200 loss 0.12651
07/09/2022 07:08:19 - INFO - __main__ -   epoch 2 step 2300 loss 0.12658
07/09/2022 07:13:40 - INFO - __main__ -   epoch 2 step 2400 loss 0.12635
07/09/2022 07:19:01 - INFO - __main__ -   epoch 2 step 2500 loss 0.12433
07/09/2022 07:24:22 - INFO - __main__ -   epoch 2 step 2600 loss 0.12621
07/09/2022 07:27:55 - INFO - __main__ -   ***** Running evaluation *****
07/09/2022 07:27:55 - INFO - __main__ -     Num examples = 8000
07/09/2022 07:27:55 - INFO - __main__ -     Batch size = 16
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
07/09/2022 07:40:16 - INFO - __main__ -     eval_loss = 0.2245
07/09/2022 07:40:16 - INFO - __main__ -     eval_map = 0.7069
07/09/2022 07:40:16 - INFO - __main__ -     ********************
07/09/2022 07:40:16 - INFO - __main__ -     Best map:0.7069
07/09/2022 07:40:16 - INFO - __main__ -     ********************
07/09/2022 07:40:17 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-map/model.bin
07/09/2022 07:40:18 - INFO - __main__ -   ***** Running evaluation *****
07/09/2022 07:40:18 - INFO - __main__ -     Num examples = 8000
07/09/2022 07:40:18 - INFO - __main__ -     Batch size = 16
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
07/09/2022 07:52:39 - INFO - __main__ -   ***** Eval results *****
07/09/2022 07:52:39 - INFO - __main__ -     eval_loss = 0.2261
07/09/2022 07:52:39 - INFO - __main__ -     eval_map = 0.7069
07/09/2022 07:52:52 - INFO - __main__ -   ***** Running Test *****
07/09/2022 07:52:52 - INFO - __main__ -     Num examples = 12000
07/09/2022 07:52:52 - INFO - __main__ -     Batch size = 16
