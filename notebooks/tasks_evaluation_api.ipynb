{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bba13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XinZhou\\anaconda3\\envs\\dna\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\XinZhou\\anaconda3\\envs\\dna\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "from datasets import Dataset\n",
    "import pyarrow as pa\n",
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from tokenizers.processors import BertProcessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45c6a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading for different datasets\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "max_step_saves = 3\n",
    "test_only = False\n",
    "data_name = 'devign' # poj, devign, draper, d2a, mvdsc, sysevr\n",
    "truncate_longer_samples = True\n",
    "# is_windows = sys.platform.startswith('win')\n",
    "# if is_windows:\n",
    "\n",
    "# data_dir = \"..\\\\..\\\\cbert\\\\dna_data_rename_tag\"\n",
    "data_dir = \"..\\\\..\\\\cbert\\\\test_data\"\n",
    "out_dir = 'result_{}'.format(data_name)\n",
    "\n",
    "# bert_model_path = '..\\\\..\\\\cbert\\\\pretrained\\\\test_albert_NaCUT\\\\checkpoint'\n",
    "bert_model_path = '..\\\\pretrained-dna-bert\\\\best'\n",
    "tokenizer_path = '..\\\\WordPiece_tokenizer'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "200accfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done...\n",
      "Dataset({\n",
      "    features: ['filename', 'text', 'label'],\n",
      "    num_rows: 800\n",
      "})\n",
      "Dataset({\n",
      "    features: ['filename', 'text', 'label'],\n",
      "    num_rows: 100\n",
      "})\n",
      "Dataset({\n",
      "    features: ['filename', 'text', 'label'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    if not text:\n",
    "        return 'None'\n",
    "    for s in ['(', ')', ':']:\n",
    "        text = text.split(s)\n",
    "        replace = ' {} '.format(s)\n",
    "        text = replace.join(text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_DNa_data(filename, mode='code'):\n",
    "    df = {'filename': [], 'text': [], 'label': []}\n",
    "    with open(filename, 'rb') as f:\n",
    "        mydict = pickle.load(f)\n",
    "        #print('number of rows:', len(mydict['filename']))\n",
    "        for i in range(len(mydict['filename'][:1000])):\n",
    "            text = mydict[mode][i]\n",
    "            text = preprocess(text)\n",
    "            df['filename'].append(mydict['filename'][i])\n",
    "            df['text'].append(text)\n",
    "            df['label'].append(mydict['label'][i])\n",
    "    print('done...')\n",
    "    df = pd.DataFrame(df)\n",
    "    ### convert to Huggingface dataset\n",
    "    hg_dataset = Dataset(pa.Table.from_pandas(df))\n",
    "    return hg_dataset\n",
    "\n",
    "# train_data = get_data('/scratch/xin/bert_source/mvdsc/tokenized_mvdsc_train.pkl')\n",
    "# valid_data = get_data('/scratch/xin/bert_source/mvdsc/tokenized_mvdsc_valid.pkl')\n",
    "if data_name == 'devign':\n",
    "    raw_data_path = os.path.join(data_dir, data_name, 'devign_all.pkl')\n",
    "    source_code_data = load_DNa_data(raw_data_path, mode='code')\n",
    "    data = source_code_data.train_test_split(test_size=0.2)\n",
    "    train_data = data['train']\n",
    "    other_data = data['test']\n",
    "    split_data = other_data.train_test_split(test_size=0.5)\n",
    "    valid_data = split_data['train']\n",
    "    test_data = split_data['test']\n",
    "\n",
    "    \n",
    "elif data_name == 'mvdsc':\n",
    "    raw_data_path = os.path.join(data_dir, data_name, 'mvdsc_train.pkl')\n",
    "    train_data = load_DNa_data(raw_data_path, mode='code')\n",
    "    raw_data_path = os.path.join(data_dir, data_name, 'mvdsc_valid.pkl')\n",
    "    valid_data = load_DNa_data(raw_data_path, mode='code')\n",
    "    raw_data_path = os.path.join(data_dir, data_name, 'mvdsc_test.pkl')\n",
    "    test_data = load_DNa_data(raw_data_path, mode='code')\n",
    "\n",
    "\n",
    "\n",
    "# train_data = train_data[:100]\n",
    "# valid_data = valid_data[:100]\n",
    "# test_data = test_data[:100]\n",
    "print(train_data)\n",
    "print(valid_data)\n",
    "print(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "540c7751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file ..\\WordPiece_tokenizer\\config.json\n",
      "Model config BertConfig {\n",
      "  \"BOS_token\": \"<s>\",\n",
      "  \"EOS\": \"</s>\",\n",
      "  \"_name_or_path\": \"..\\\\WordPiece_tokenizer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Didn't find file ..\\WordPiece_tokenizer\\tokenizer.json. We won't load it.\n",
      "Didn't find file ..\\WordPiece_tokenizer\\added_tokens.json. We won't load it.\n",
      "Didn't find file ..\\WordPiece_tokenizer\\special_tokens_map.json. We won't load it.\n",
      "Didn't find file ..\\WordPiece_tokenizer\\tokenizer_config.json. We won't load it.\n",
      "loading file ..\\WordPiece_tokenizer\\vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ..\\WordPiece_tokenizer\\config.json\n",
      "Model config BertConfig {\n",
      "  \"BOS_token\": \"<s>\",\n",
      "  \"EOS\": \"</s>\",\n",
      "  \"_name_or_path\": \"..\\\\WordPiece_tokenizer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Adding [SEP] to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file ..\\WordPiece_tokenizer\\config.json\n",
      "Model config BertConfig {\n",
      "  \"BOS_token\": \"<s>\",\n",
      "  \"EOS\": \"</s>\",\n",
      "  \"_name_or_path\": \"..\\\\WordPiece_tokenizer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\WordPiece_tokenizer\n",
      "..\\WordPiece_tokenizer\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "print(tokenizer_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "print(tokenizer_path)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9409bb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.48ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.20ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.63ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['filename', 'text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 800\n",
      "})\n",
      "Dataset({\n",
      "    features: ['filename', 'text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 100\n",
      "})\n",
      "Dataset({\n",
      "    features: ['filename', 'text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 100\n",
      "})\n",
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_train_data = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_valid_data = valid_data.map(preprocess_function, batched=True)\n",
    "tokenized_test_data = test_data.map(preprocess_function, batched=True)\n",
    "\n",
    "print(tokenized_train_data)\n",
    "print(tokenized_valid_data)\n",
    "print(tokenized_test_data)\n",
    "print(len(tokenized_train_data[1]['input_ids']))\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "# train_dataloader = DataLoader(train_data, shuffle=True, batch_size=8)\n",
    "# valid_dataloader = DataLoader(valid_data, batch_size=8)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e01154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d60c19dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ..\\pretrained-dna-bert\\best\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"..\\\\pretrained-dna-bert\\\\best\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8929\n",
      "}\n",
      "\n",
      "loading weights file ..\\pretrained-dna-bert\\best\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at ..\\pretrained-dna-bert\\best were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ..\\pretrained-dna-bert\\best and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import DataCollatorWithPadding\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(bert_model_path, num_labels=2)\n",
    "\n",
    "# import wandb\n",
    "\n",
    "# wandb.init(project=\"test-project\", entity=\"xinzhou\")\n",
    "# # wandb.config = {\n",
    "# #   \"learning_rate\": 0.001,\n",
    "# #   \"epochs\": 100,\n",
    "# #   \"batch_size\": 128\n",
    "# # }\n",
    "# # wandb.log({\"loss\": loss})\n",
    "\n",
    "# # Optional\n",
    "# # wandb.watch(model)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(bert_model_path, num_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7778bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 100\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: filename, text. If filename, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 36\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/36 00:23 < 00:29, 0.65 it/s, Epoch 1.32/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "output_dir=\"result_{}\".format(data_name)\n",
    "\n",
    "\n",
    "batch_size_per_gpu = 8\n",
    "save_step = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True, \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size_per_gpu,\n",
    "    per_device_eval_batch_size=batch_size_per_gpu,\n",
    "    gradient_accumulation_steps=8, # accumulating the gradients before updating the weights\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=save_step,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=save_step,\n",
    "    load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    save_total_limit=5,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_valid_data,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "PATH = os.path.join(output_dir, 'checkpoint-best')\n",
    "\n",
    "trainer.save_model(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7654e9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: filename, text. If filename, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "predict_output = trainer.predict(test_dataset=tokenized_test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4e29483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 1 1 0 1 1 1 0]\n",
      "Test Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# predictions, label_ids, metrics\n",
    "predictions = predict_output[0]\n",
    "labels = predict_output[1]\n",
    "\n",
    "# print(type(predictions), predictions.shape)\n",
    "predicted_values = np.argmax(predictions, 1)\n",
    "\n",
    "print(predicted_values[:100])\n",
    "print(labels[:100])\n",
    "test_accuracy = np.sum(predicted_values == labels) / len(labels)\n",
    "\n",
    "print (\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27507e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e75ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ba9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d075664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19161df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7f7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe6c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2efc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02640a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48ae55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a28f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8ca24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7865b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64535cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b99602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a9f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a3988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c41932a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# plot confusion matrix\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# code borrowed from scikit-learn.org\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_confusion_matrix\u001b[39m(cm, classes,\n\u001b[0;32m      6\u001b[0m                           normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m                           title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfusion matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m----> 8\u001b[0m                           cmap\u001b[38;5;241m=\u001b[39mplt\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mBlues):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    This function prints and plots the confusion matrix.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    Normalization can be applied by setting `normalize=True`.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# import itertools\n",
    "\n",
    "# # plot confusion matrix\n",
    "# # code borrowed from scikit-learn.org\n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "#     \"\"\"\n",
    "#     This function prints and plots the confusion matrix.\n",
    "#     Normalization can be applied by setting `normalize=True`.\n",
    "#     \"\"\"\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, format(cm[i, j], fmt),\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "653c3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# cm_test = confusion_matrix(true_values, predicted_values)\n",
    "\n",
    "# np.set_printoptions(precision=2)\n",
    "\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plot_confusion_matrix(cm_test, classes=label_values, title='Confusion Matrix - Test Dataset')\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plot_confusion_matrix(cm_test, classes=label_values, title='Confusion Matrix - Test Dataset', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5d314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c4989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
