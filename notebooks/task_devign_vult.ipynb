{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "from tokenizers import *\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "# from tokenizers.processors import BertProcessing\n",
    "import numpy as np\n",
    "import models\n",
    "from models import DefectModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m log_txt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_dir: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(data_dir))\n\u001b[1;32m     48\u001b[0m log_txt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_dir: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(out_dir))\n\u001b[0;32m---> 49\u001b[0m log_txt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert_model_path: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mbert_model_path\u001b[49m))\n\u001b[1;32m     50\u001b[0m log_txt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer_path: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(tokenizer_path))\n\u001b[1;32m     51\u001b[0m log_txt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAX_LENGTH: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(MAX_LENGTH))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bert_model_path' is not defined"
     ]
    }
   ],
   "source": [
    "# INPORTANT\n",
    "# out_dir = './task_devign_v3'\n",
    "# raw_data_path = '/scratch/xin/devign_task/tokenized_devign_v3.pkl'\n",
    "# model_path = '/scratch/xin/bert_source_v3/pretrained-bert/'\n",
    "num_epochs = 3\n",
    "max_step_saves = 3\n",
    "save_step_at = 1000\n",
    "learning_rate = 5e-5\n",
    "adam_epsilon = 1e-8\n",
    "early_stop = 10\n",
    "normalize = 'ReLU'\n",
    "t5_checkpoint = \"t5-base\"\n",
    "test_only = False\n",
    "data_type = 'devign'\n",
    "\n",
    "# # redas-lab2\n",
    "MAX_LENGTH = 512\n",
    "data_dir = \"/scratch/dna_data_vult/\"\n",
    "vult_model_path = '../pretrained-dna-vult/checkpoint-80'\n",
    "tokenizer_path = \"Salesforce/codet5-base\"\n",
    "batch_size = 2  # 8GB per GPU   \n",
    "\n",
    "# # # carya cluster\n",
    "# MAX_LENGTH = 1024\n",
    "# data_dir = \"../../dna_data_lite_2/{}\".format(data_type)\n",
    "# bert_model_path = '../pretrained-dna-roberta-2/checkpoint-40000'\n",
    "# tokenizer_path = '../../dna_data_pretraining_2/BPE_tokenizer'\n",
    "# batch_size = 16 # 32GB per GPU -  max_length 512: 32 -max_length 1024: 16\n",
    "\n",
    "\n",
    "out_dir = '../result_duo_mode_{}_{}_with_tagging'.format(data_type, MAX_LENGTH)\n",
    "truncate_longer_samples = True\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "log_txt_file = os.path.join(out_dir, 'log.txt')\n",
    "# intialization\n",
    "with open(log_txt_file, 'w+') as f:\n",
    "    f.write('- log -\\n')\n",
    "\n",
    "def log_txt(s, type='normal'):\n",
    "    with open(log_txt_file, 'a') as f:\n",
    "        f.write('{} - {}\\n'.format( type, s))\n",
    "\n",
    "# intial parameters\n",
    "log_txt('data_type: {}'.format(data_type))\n",
    "log_txt('data_dir: {}'.format(data_dir))\n",
    "log_txt('out_dir: {}'.format(out_dir))\n",
    "log_txt('bert_model_path: {}'.format(bert_model_path))\n",
    "log_txt('tokenizer_path: {}'.format(tokenizer_path))\n",
    "log_txt('MAX_LENGTH: {}'.format(MAX_LENGTH))\n",
    "log_txt('batch_size: {}'.format(batch_size))\n",
    "log_txt('num_epochs: {}'.format(num_epochs))\n",
    "log_txt('save_step_at: {}'.format(save_step_at))\n",
    "log_txt('learning_rate: {}'.format(learning_rate))\n",
    "log_txt('adam_epsilon: {}'.format(adam_epsilon))\n",
    "log_txt('normalize: {}'.format(normalize))\n",
    "log_txt('CLASSIFIER_HIDDEN_SIZE: {}'.format(CLASSIFIER_HIDDEN_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data(file_path):\n",
    "    raw_df = pd.read_pickle(file_path)  \n",
    "    df = {'filename': [], 'code': [], 'nat': [], 'tags': [], 'label': []}\n",
    "    for i in range(len(raw_df['code'][:500])):\n",
    "        df['filename'].append(raw_df['filename'][i])\n",
    "        ## add \"Defect: \" is required to switch VulT model for specific task\n",
    "        df['code'].append('Defect: ' + raw_df['code'][i])\n",
    "        df['nat'].append('Defect: ' +raw_df['nat'][i])\n",
    "        df['tags'].append('Defect: ' +raw_df['tags'][i])\n",
    "        df['label'].append(raw_df['label'][i])\n",
    "        \n",
    "    df = pd.DataFrame(df)\n",
    "    return df\n",
    "    ### convert to Huggingface dataset\n",
    "#     return Dataset(pa.Table.from_pandas(df))\n",
    "\n",
    "# random split into train/valid/test\n",
    "if data_type in ['devign']:\n",
    "    train_data = get_data(os.path.join(data_dir, 'devign_train.pkl'))\n",
    "    valid_data = get_data(os.path.join(data_dir, 'devign_valid.pkl'))\n",
    "    test_data = get_data(os.path.join(data_dir, 'devign_test.pkl'))\n",
    "# # load train/valid/test\n",
    "# elif data_type in ['mvdsc', 'd2a' ]:\n",
    "#     train_data = get_data(os.path.join(data_dir, 'train.pkl'))\n",
    "#     valid_data = get_data(os.path.join(data_dir, 'valid.pkl'))\n",
    "#     test_data = get_data(os.path.join(data_dir, 'test.pkl'))\n",
    "# # load train/valid/test from 4 types\n",
    "# elif data_type in ['sysevr']:\n",
    "#     # already randomized from pre-processing with fixed seed for pre-training and fine-tuning\n",
    "#     raw_data_path = os.path.join(data_dir, 'api.pkl')\n",
    "#     tokenized_data = get_data(raw_data_path)\n",
    "#     train_data_1, valid_data_1, test_data_1 = np.split(tokenized_data, [int(.7*len(tokenized_data)), int(.85*len(tokenized_data))])\n",
    "#     raw_data_path = os.path.join(data_dir, 'au.pkl')\n",
    "#     tokenized_data = get_data(raw_data_path)\n",
    "#     train_data_2, valid_data_2, test_data_2 = np.split(tokenized_data, [int(.7*len(tokenized_data)), int(.85*len(tokenized_data))])\n",
    "#     raw_data_path = os.path.join(data_dir, 'ae.pkl')\n",
    "#     tokenized_data = get_data(raw_data_path)\n",
    "#     train_data_3, valid_data_3, test_data_3 = np.split(tokenized_data, [int(.7*len(tokenized_data)), int(.85*len(tokenized_data))])\n",
    "#     raw_data_path = os.path.join(data_dir, 'pu.pkl')\n",
    "#     tokenized_data = get_data(raw_data_path)\n",
    "#     train_data_4, valid_data_4, test_data_4 = np.split(tokenized_data, [int(.7*len(tokenized_data)), int(.85*len(tokenized_data))])\n",
    "#     tokenized_data = get_data(raw_data_path)\n",
    "#     train_data = pd.concat([train_data_1, train_data_2, train_data_3, train_data_4], ignore_index=True)\n",
    "#     valid_data = pd.concat([valid_data_1, valid_data_2, valid_data_3, valid_data_4], ignore_index=True)\n",
    "#     test_data = pd.concat([test_data_1, test_data_2, test_data_3, test_data_4], ignore_index=True)\n",
    "# elif  data_type in ['draper']:\n",
    "#     # splitted data bc too large\n",
    "#     all_data = {}\n",
    "#     for i in range(10):\n",
    "#         raw_data_path = os.path.join(data_dir, 'train_{}.pkl'.format(i))\n",
    "#         tokenized_data = get_data(raw_data_path)\n",
    "#         #train_data = pd.concat([train_data_1, train_data_2, train_data_3, train_data_4], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    print('data type does not exist!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 5)\n",
      "(500, 5)\n",
      "(500, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)\n",
    "# print(train_data['code'][:3])\n",
    "log_txt('train: {} - valid: {} - test: {} '.format(len(train_data), len(valid_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_code = train_data['code'].values\n",
    "train_nat = train_data['nat'].values\n",
    "train_tags = train_data['tags'].values\n",
    "train_labels = train_data['label'].values\n",
    "\n",
    "valid_code = valid_data['code'].values\n",
    "valid_nat = valid_data['nat'].values\n",
    "valid_tags = valid_data['tags'].values\n",
    "valid_labels = valid_data['label'].values\n",
    "\n",
    "test_code = test_data['code'].values\n",
    "test_nat = test_data['nat'].values\n",
    "test_tags = test_data['tags'].values\n",
    "test_labels = test_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "train_texts = train_code\n",
    "log_txt('---------- train_code ---------------')\n",
    "text_lengths = [len(train_texts[i].split()) for i in range(len(train_texts))]\n",
    "log_txt('min: {}'.format(min(text_lengths)))\n",
    "log_txt('min: {}'.format(min(text_lengths)))\n",
    "log_txt('number of zeros:: {}'.format(len([item for item in text_lengths if item == 0])))\n",
    "log_txt('max: {}'.format(max(sorted(text_lengths))))\n",
    "log_txt('avg: {}'.format( sum(text_lengths) / len(text_lengths) ))\n",
    "log_txt('median: {}'.format( sorted(text_lengths)[len(text_lengths) // 2] ))\n",
    "log_txt('95%: {}'.format( sorted(text_lengths)[int(len(text_lengths)*0.95)]))\n",
    "log_txt('mean: {}'.format( statistics.mean(text_lengths)))\n",
    "log_txt('std: {}'.format( statistics.stdev(text_lengths)))\n",
    "\n",
    "train_texts = train_nat\n",
    "log_txt('---------- train_nat ---------------')\n",
    "text_lengths = [len(train_texts[i].split()) for i in range(len(train_texts))]\n",
    "log_txt('min: {}'.format(min(text_lengths)))\n",
    "log_txt('min: {}'.format(min(text_lengths)))\n",
    "log_txt('number of zeros:: {}'.format(len([item for item in text_lengths if item == 0])))\n",
    "log_txt('max: {}'.format(max(sorted(text_lengths))))\n",
    "log_txt('avg: {}'.format( sum(text_lengths) / len(text_lengths) ))\n",
    "log_txt('median: {}'.format( sorted(text_lengths)[len(text_lengths) // 2] ))\n",
    "log_txt('95%: {}'.format( sorted(text_lengths)[int(len(text_lengths)*0.95)]))\n",
    "log_txt('mean: {}'.format( statistics.mean(text_lengths)))\n",
    "log_txt('std: {}'.format( statistics.stdev(text_lengths)))\n",
    "\n",
    "\n",
    "train_texts = train_tags\n",
    "log_txt('---------- train_tags ---------------')\n",
    "text_lengths = [len(train_texts[i].split()) for i in range(len(train_texts))]\n",
    "log_txt('min: {}'.format(min(text_lengths)))\n",
    "log_txt('min: {}'.format(min(text_lengths)))\n",
    "log_txt('number of zeros:: {}'.format(len([item for item in text_lengths if item == 0])))\n",
    "log_txt('max: {}'.format(max(sorted(text_lengths))))\n",
    "log_txt('avg: {}'.format( sum(text_lengths) / len(text_lengths) ))\n",
    "log_txt('median: {}'.format( sorted(text_lengths)[len(text_lengths) // 2] ))\n",
    "log_txt('95%: {}'.format( sorted(text_lengths)[int(len(text_lengths)*0.95)]))\n",
    "log_txt('mean: {}'.format( statistics.mean(text_lengths)))\n",
    "log_txt('std: {}'.format( statistics.stdev(text_lengths)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_604/280788245.py:8: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(text_lengths, hist=True, kde=False,\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# matplotlib histogram\n",
    "plt.hist(text_lengths, color = 'blue', edgecolor = 'black',\n",
    "         bins = int(180/5))\n",
    "# seaborn histogram\n",
    "sns.distplot(text_lengths, hist=True, kde=False, \n",
    "             bins=int(180/5), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "# Add labels\n",
    "plt.title('Histogram of text length analysis')\n",
    "plt.xlabel('text length')\n",
    "plt.ylabel('counts')\n",
    "plt.savefig(os.path.join(out_dir, 'histogram.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salesforce/codet5-base\n"
     ]
    }
   ],
   "source": [
    "# ========================= load tokenizer START =========================\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "# vocab_size = 32010\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "import sentencepiece as spm\n",
    "import transformers \n",
    "# !pip install sentencepiece\n",
    "print(tokenizer_path)\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "if 'codet5' in tokenizer_path:\n",
    "    tokenizer = transformers.RobertaTokenizer.from_pretrained(tokenizer_path)\n",
    "elif 'albert' in tokenizer_path:\n",
    "    tokenizer = transformers.AlbertTokenizer.from_pretrained(tokenizer_path)\n",
    "else:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "log_txt('tokenizer_path: {}\\n'.format( tokenizer_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"text\"], max_length=max_length, padding=True, truncation=True)\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"label\"], max_length=max_length, padding=True, truncation=True)\n",
    "    model_inputs[\"decoder_input_ids\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]    \n",
    "    # model_inputs[\"labels\"] =  labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "##### train data\n",
    "train_code_dict = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_special_tokens_mask=True) for t in train_code]\n",
    "train_nat_dict = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_special_tokens_mask=True) for t in train_nat]\n",
    "train_tags_dict = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_special_tokens_mask=True) for t in train_tags]\n",
    "train_code_ids = [text_id['input_ids'] for text_id in train_code_dict]\n",
    "train_code_masks = [text_id['attention_mask'] for text_id in train_code_dict]\n",
    "train_nat_ids = [text_id['input_ids'] for text_id in train_nat_dict]\n",
    "train_nat_masks = [text_id['attention_mask'] for text_id in train_nat_dict]\n",
    "train_tag_ids = [text_id['input_ids'] for text_id in train_tags_dict]\n",
    "train_tag_masks = [text_id['attention_mask'] for text_id in train_tags_dict]\n",
    "\n",
    "train_x1, train_m1, train_x2, train_m2, train_x3, train_m3, train_y = train_code_ids, train_code_masks, train_nat_ids, train_nat_masks, train_tag_ids, train_tag_masks, train_labels\n",
    "train_x1 = torch.tensor(train_x1)\n",
    "train_m1 = torch.tensor(train_m1)\n",
    "train_x2 = torch.tensor(train_x2)\n",
    "train_m2 = torch.tensor(train_m2)\n",
    "train_x3 = torch.tensor(train_x3)\n",
    "train_m3 = torch.tensor(train_m3)\n",
    "train_y = torch.tensor(train_y)\n",
    "\n",
    "train_data = TensorDataset(train_x1, train_m1, train_x2, train_m2, train_x3, train_m3, train_y)\n",
    "train_sampler = list(range(len(train_data)))\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "#### valid data\n",
    "valid_code_dict = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_special_tokens_mask=True) for t in valid_code]\n",
    "valid_nat_dict = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_special_tokens_mask=True) for t in valid_nat]\n",
    "valid_tags_dict = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_special_tokens_mask=True) for t in valid_tags]\n",
    "valid_code_ids = [text_id['input_ids'] for text_id in valid_code_dict]\n",
    "valid_code_masks = [text_id['attention_mask'] for text_id in valid_code_dict]\n",
    "valid_nat_ids = [text_id['input_ids'] for text_id in valid_nat_dict]\n",
    "valid_nat_masks = [text_id['attention_mask'] for text_id in valid_nat_dict]\n",
    "valid_tag_ids = [text_id['input_ids'] for text_id in valid_tags_dict]\n",
    "valid_tag_masks = [text_id['attention_mask'] for text_id in valid_tags_dict]\n",
    "\n",
    "valid_x1, valid_m1, valid_x2, valid_m2, valid_x3, valid_m3, valid_y = valid_code_ids, valid_code_masks, valid_nat_ids, valid_nat_masks, valid_tag_ids, valid_tag_masks, valid_labels\n",
    "valid_x1 = torch.tensor(valid_x1)\n",
    "valid_m1 = torch.tensor(valid_m1)\n",
    "valid_x2 = torch.tensor(valid_x2)\n",
    "valid_m2 = torch.tensor(valid_m2)\n",
    "valid_x3 = torch.tensor(valid_x3)\n",
    "valid_m3 = torch.tensor(valid_m3)\n",
    "valid_y = torch.tensor(valid_y)\n",
    "\n",
    "valid_data = TensorDataset(valid_x1, valid_m1, valid_x2, valid_m2, valid_x3, valid_m3, valid_y)\n",
    "valid_sampler = list(range(len(valid_data)))\n",
    "valid_sampler = RandomSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n",
    "\n",
    "#### test data\n",
    "test_code_dict = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_special_tokens_mask=True) for t in test_code]\n",
    "test_nat_dict = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_special_tokens_mask=True) for t in test_nat]\n",
    "test_tags_dict = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_special_tokens_mask=True) for t in test_tags]\n",
    "test_code_ids = [text_id['input_ids'] for text_id in test_code_dict]\n",
    "test_code_masks = [text_id['attention_mask'] for text_id in test_code_dict]\n",
    "test_nat_ids = [text_id['input_ids'] for text_id in test_nat_dict]\n",
    "test_nat_masks = [text_id['attention_mask'] for text_id in test_nat_dict]\n",
    "test_tag_ids = [text_id['input_ids'] for text_id in test_tags_dict]\n",
    "test_tag_masks = [text_id['attention_mask'] for text_id in test_tags_dict]\n",
    "\n",
    "test_x1, test_m1, test_x2, test_m2, test_x3, test_m3, test_y = test_code_ids, test_code_masks, test_nat_ids, test_nat_masks, test_tag_ids, test_tag_masks, test_labels\n",
    "test_x1 = torch.tensor(test_x1)\n",
    "test_m1 = torch.tensor(test_m1)\n",
    "test_x2 = torch.tensor(test_x2)\n",
    "test_m2 = torch.tensor(test_m2)\n",
    "test_x3 = torch.tensor(test_x3)\n",
    "test_m3 = torch.tensor(test_m3)\n",
    "test_y = torch.tensor(test_y)\n",
    "\n",
    "test_data = TensorDataset(test_x1, test_m1, test_x2, test_m2, test_x3, test_m3, test_y)\n",
    "test_sampler = list(range(len(test_data)))\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "\n",
    "\n",
    "# MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer),\n",
    "#                  't5': (T5Config, T5ForConditionalGeneration, T5Tokenizer),\n",
    "#                  'codet5': (T5Config, T5ForConditionalGeneration, RobertaTokenizer),\n",
    "#                  'bart': (BartConfig, BartForConditionalGeneration, BartTokenizer),\n",
    "#                  'vult': (T5Config, T5ForConditionalGeneration, BartTokenizer),\n",
    "#                 }\n",
    "\n",
    "class VulT6_Classifier(nn.Module):\n",
    "    def __init__(self, encoder, tokenizer, config, num_labels): \n",
    "        super(VulT6_Classifier,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "        self.encoder = encoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        \n",
    "    def get_vec(self, source_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=source_ids, attention_mask=attention_mask,\n",
    "                               labels=source_ids, decoder_attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs['decoder_hidden_states'][-1]\n",
    "        eos_mask = source_ids.eq(self.config.eos_token_id)\n",
    "        if len(torch.unique(eos_mask.sum(1))) > 1:\n",
    "            raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n",
    "        vec = hidden_states[eos_mask, :].view(hidden_states.size(0), -1,\n",
    "                                              hidden_states.size(-1))[:, -1, :]\n",
    "        return vec\n",
    "    \n",
    "    def forward(self, x1, m1, x2, m2, x3, m3, labels=None):\n",
    "        vec_1 = self.get_vec(x1, m1)\n",
    "        # batch_size x 768 (hidden_size)\n",
    "        # print(vec_code.shape)\n",
    "        \n",
    "        logits = self.classifier(vec_1)\n",
    "        prob = nn.functional.softmax(logits)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return (loss, logits), None\n",
    "        #return (loss, final_logits), final_attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_txt(vult_model_path)\n",
    "encoder_model = transformers.T5ForConditionalGeneration.from_pretrained(vult_model_path)\n",
    "model_config = transformers.T5Config.from_pretrained(vult_model_path)    \n",
    "\n",
    "model = VulT6_Classifier(encoder_model, tokenizer, model_config, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.2},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "multi_gpu = torch.cuda.device_count() \n",
    "if multi_gpu > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = nn.DataParallel(model)\n",
    "# model = nn.DataParallel(model)\n",
    "model = model.cuda()\n",
    "\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations:  250\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed_val = 111\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "import pickle\n",
    "import os\n",
    "# save model \n",
    "def save_model(model, train_losses, val_losses, out_dir_model):\n",
    "    if not os.path.exists(out_dir_model):\n",
    "        os.makedirs(out_dir_model)\n",
    "    PATH = os.path.join(out_dir_model, 'VulTBinaryClassification.pt')\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    with open(out_dir_model + '/train_losses.pkl', 'wb') as f:\n",
    "        pickle.dump(train_losses, f)\n",
    "    with open(out_dir_model + '/val_losses.pkl', 'wb') as f:\n",
    "        pickle.dump(val_losses, f)\n",
    "print('number of iterations: ', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_604/1081882820.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = nn.functional.softmax(logits)\n",
      "/scratch/conda_envs/dna/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "250it [06:51,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss after itaration 1: 3.620058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [02:16,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after itaration 1: 2.425647\n",
      "val_losses:  [] 2.4256465435028076\n",
      "Time: 9m 8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:52,  1.63s/it]"
     ]
    }
   ],
   "source": [
    "if not test_only:\n",
    "    from tqdm import tqdm\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    num_mb_train = len(train_dataloader)\n",
    "    num_mb_val = len(valid_dataloader)\n",
    "    current_step = 0\n",
    "    saved_steps = []\n",
    "    if num_mb_val == 0:\n",
    "        num_mb_val = 1\n",
    "\n",
    "    # print('number of iterations: ', len(train_dataloader), len(train_loader_extracted))\n",
    "    print('num_epochs: ', num_epochs)\n",
    "\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    for n in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        #for k, (data1, data2) in enumerate(zip(train_dataloader, train_loader_extracted)):\n",
    "        for k, (x1, m1, x2, m2, x3, m3, y) in tqdm(enumerate(train_dataloader)):\n",
    "            #mb_x, mb_m, mb_y = data1\n",
    "            # 56 features\n",
    "            #ex, ey, e_index = data2\n",
    "            optimizer.zero_grad()\n",
    "            model.train()\n",
    "            x1 = x1.cuda()\n",
    "            m1 = m1.cuda()\n",
    "            x2 = x2.cuda()\n",
    "            m2 = m2.cuda()\n",
    "            x3 = x3.cuda()\n",
    "            m3 = m3.cuda()\n",
    "            y = y.cuda()\n",
    "            outputs, _, = model(x1, m1, x2, m2, x3, m3, labels=y)\n",
    "            #print(outputs[1])\n",
    "            #mb_y = F.one_hot(mb_y)\n",
    "            #print(outputs, mb_y)\n",
    "            #loss = loss_fct(outputs.view(-1, 2), mb_y.view(-1))\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if multi_gpu > 1:\n",
    "                loss = loss.sum()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            current_step += 1\n",
    "            train_loss += loss.data / num_mb_train\n",
    "\n",
    "            if current_step % save_step_at == 0:\n",
    "                saved_steps.append(current_step)\n",
    "                out_dir_model = os.path.join(out_dir, 'classification_model_{}'.format(saved_steps[-1]) )\n",
    "                print('saved as: ', out_dir_model)\n",
    "                if len(saved_steps) > max_step_saves:\n",
    "                    delete_dir = os.path.join(out_dir, 'classification_model_{}'.format(saved_steps[-(max_step_saves+1)]))\n",
    "                    os.system('rm -r {}'.format(delete_dir))\n",
    "                save_model(model, train_losses, val_losses, out_dir_model)\n",
    "\n",
    "\n",
    "        print (\"\\nTrain loss after itaration %i: %f\" % (n+1, train_loss))\n",
    "        train_losses.append(train_loss.cpu())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for k, (x1, m1, x2, m2, x3, m3, y) in tqdm(enumerate(valid_dataloader)):\n",
    "                x1 = x1.cuda()\n",
    "                m1 = m1.cuda()\n",
    "                x2 = x2.cuda()\n",
    "                m2 = m2.cuda()\n",
    "                x3 = x3.cuda()\n",
    "                m3 = m3.cuda()\n",
    "                y = y.cuda()\n",
    "                outputs, _, = model(x1, m1, x2, m2, x3, m3, labels=y)\n",
    "                \n",
    "                loss = outputs[0]\n",
    "                if multi_gpu > 1:\n",
    "                    loss = loss.sum()\n",
    "                #loss = model_loss(outputs[1], mb_y)\n",
    "\n",
    "                val_loss += loss.data / num_mb_val\n",
    "\n",
    "            print (\"Validation loss after itaration %i: %f\" % (n+1, val_loss))\n",
    "            # save the best\n",
    "\n",
    "            print('val_losses: ', val_losses, float(val_loss.cpu()))\n",
    "            if val_losses and float(val_loss.cpu()) <= min(val_losses):\n",
    "                out_dir_model = os.path.join(out_dir, 'classification_model_best')\n",
    "                print('the best model updated')\n",
    "                print('saved as: ', out_dir_model)\n",
    "                save_model(model, train_losses, val_losses, out_dir_model)\n",
    "            val_losses.append(float(val_loss.cpu()))\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        print(f'Time: {epoch_mins}m {epoch_secs}s')\n",
    "        if len(val_losses) > 3 and float(val_loss.cpu()) > max(val_losses[-4:-1]):\n",
    "            break\n",
    "\n",
    "\n",
    "    out_dir_model = os.path.join(out_dir, 'classification_model_last')\n",
    "    save_model(model, train_losses, val_losses, out_dir_model)\n",
    "    print('model saved as: ', out_dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# out_dir_model = '{}/classification_model_4000'.format(out_dir)\n",
    "\n",
    "test_dir_model = '{}/classification_model_last'.format(out_dir)\n",
    "\n",
    "PATH = os.path.join(test_dir_model, 'VulTBinaryClassification.pt')\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "\n",
    "with open(test_dir_model + '/train_losses.pkl', 'rb') as f:\n",
    "    train_losses = pickle.load(f)\n",
    "    \n",
    "with open(test_dir_model + '/val_losses.pkl', 'rb') as f:\n",
    "    val_losses = pickle.load(f)\n",
    "\n",
    "print(val_losses)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.savefig(os.path.join(out_dir, 'train_valid_loss.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "print('num of iterations: ', len(test_dataloader))\n",
    "with torch.no_grad():\n",
    "    for k, (x1, m1, x2, m2, x3, m3, y) in tqdm(enumerate(test_dataloader)):\n",
    "        x1 = x1.cuda()\n",
    "        m1 = m1.cuda()\n",
    "        x2 = x2.cuda()\n",
    "        m2 = m2.cuda()\n",
    "        x3 = x3.cuda()\n",
    "        m3 = m3.cuda()\n",
    "        y = y.cuda()\n",
    "        output, _, = model(x1, m1, x2, m2, x3, m3, labels=y)\n",
    "        outputs.append(output[1].to('cpu'))\n",
    "outputs = torch.cat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[:10])\n",
    "print(test_y[:10])\n",
    "true_values = test_y.numpy()\n",
    "\n",
    "################################################\n",
    "## OPTIONAL: to find best detection threshold ##\n",
    "results = []\n",
    "for i in range(100):\n",
    "    prediction_probs = outputs[:,1]\n",
    "    detection_threshold = i / 100\n",
    "    predictions = [1 if x >= detection_threshold else 0 for x in list(prediction_probs)]\n",
    "    test_accuracy = np.sum(predictions == true_values) / len(true_values)\n",
    "    results.append(test_accuracy)\n",
    "results = sorted(results)\n",
    "print('lowest: ', results[:5])\n",
    "print('highest: ', results[-5:])\n",
    "################################################\n",
    "\n",
    "_, predicted_values = torch.max(outputs, 1)\n",
    "predicted_values = predicted_values.numpy()\n",
    "\n",
    "test_accuracy = np.sum(predicted_values == true_values) / len(true_values)\n",
    "print (\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "log_txt(\"Test Accuracy: {}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# plot confusion matrix\n",
    "# code borrowed from scikit-learn.org\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "label_values = [0, 1]\n",
    "def confusion_matrix_scorer(y_pred, y):\n",
    "    cm = metrics.confusion_matrix(y, y_pred)\n",
    "    return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "        'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "\n",
    "cm_test = metrics.confusion_matrix(true_values, predicted_values)\n",
    "print({ 'tn': cm_test[0, 0], 'fp': cm_test[0, 1],\n",
    "        'fn': cm_test[1, 0], 'tp': cm_test[1, 1]})\n",
    "\n",
    "log_txt('tn: {} - fp: {} - fn: {} - tp: {}'.format(cm_test[0, 0], cm_test[0, 1], cm_test[1, 0], cm_test[1, 1]))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(cm_test, classes=label_values, title='Confusion Matrix - Test Dataset')\n",
    "\n",
    "plt.savefig(os.path.join(out_dir, 'confusion_matrix.png'))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(cm_test, classes=label_values, title='Confusion Matrix - Test Dataset', normalize=True)\n",
    "\n",
    "plt.savefig(os.path.join(out_dir, 'confusion_matrix_normalized.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
